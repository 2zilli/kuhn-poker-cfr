{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2pXvLPPk1XgT2nRV70wPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2zilli/kuhn-poker-cfr/blob/main/kuhn_poker_cfr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counterfactual Regret Minimization (CFR) for Kuhn Poker\n",
        "\n",
        "This notebook explores the application of Counterfactual Regret Minimization (CFR) to the game of Kuhn Poker, a simplified version of poker that is often used as a benchmark in game theory and AI research. We will implement the game logic, develop a CFR agent, and simulate games to evaluate the agent's effectiveness.\n"
      ],
      "metadata": {
        "id": "fyR9YtuDg_PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract Base Class: PokerGame\n",
        "\n",
        "The `PokerGame` class serves as an abstract base class for poker-style games. This class defines the structure and required methods that any specific poker game implementation must provide. Here's an overview of its responsibilities:\n",
        "\n",
        "- **Card Representation**: Converts card values to human-readable characters.\n",
        "- **Game State Management**: Includes methods to reset the game, check if the game state is terminal, and replicate the game state.\n",
        "- **Actions and Payoffs**: Defines methods to get available actions, perform actions, determine if the current state is a showdown, and calculate payoffs.\n",
        "\n",
        "This abstraction allows us to implement any specific poker game rules by extending this base class and providing specific implementations for these abstract methods.\n"
      ],
      "metadata": {
        "id": "5poyy5-Qhpcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class PokerGame(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for poker-style games, providing common interface and functionality.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_card_char(card_value):\n",
        "        \"\"\"Returns the character representation of a card based on its integer value.\"\"\"\n",
        "        return {11: 'J', 12: 'Q', 13: 'K'}.get(card_value, '?')\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the game to its initial state, shuffles and deals cards.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_terminal(self):\n",
        "        \"\"\"Checks if the game is at a terminal state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns the list of available actions for the current game state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def perform_action(self, action):\n",
        "        \"\"\"Performs an action and updates the game state accordingly.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_showdown(self):\n",
        "        \"\"\"Determines if the current state is a showdown.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_payoff(self):\n",
        "        \"\"\"Calculates and returns the payoff based on the current game state without altering the game's state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def clone(self):\n",
        "        \"\"\"Creates a deep copy of the current game state.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Returns a string representation of the game state.\"\"\"\n",
        "        return super().__repr__()"
      ],
      "metadata": {
        "id": "J2CiHEQd62vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kuhn Poker Game Implementation\n",
        "\n",
        "`KuhnGame` extends the abstract `PokerGame` class to provide a specific implementation for Kuhn Poker, a simplified poker variant perfect for theoretical analysis and algorithm testing. Here's what this class entails:\n",
        "\n",
        "- **Initialization and Resetting**: Sets up the game with initial configurations and shuffles the cards.\n",
        "- **Game Progression**: Manages the flow of the game by updating the state based on player actions and switching turns.\n",
        "- **Terminal State Check**: Determines if the game has reached a conclusion.\n",
        "- **Available Actions**: Lists possible actions (bet or pass) depending on the game state.\n",
        "- **Action Effects**: Updates the game state according to the chosen action, managing bets and turns.\n",
        "- **Showdown Determination**: Identifies if the game ends in a showdown where cards are compared.\n",
        "- **Payoff Calculation**: Computes the monetary outcome for each player at the game's end.\n",
        "- **State Cloning**: Supports creating a deep copy of the game state, useful for simulations and AI decision-making.\n",
        "\n",
        "This class encapsulates all the rules of Kuhn Poker and interacts with player agents to simulate gameplay.\n"
      ],
      "metadata": {
        "id": "OoKYQ0zUhvb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class KuhnGame(PokerGame):\n",
        "    \"\"\"Class representing the state and logic of a Kuhn Poker game.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the KuhnGame with default settings.\"\"\"\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the game to its initial state, shuffles and deals cards.\"\"\"\n",
        "        self.current_player = 0\n",
        "        self.history = \"\"\n",
        "        self.bets = [-1, -1]  # Each player antes 1 unit\n",
        "        self.cards = [11, 12, 13]  # Using integers for easier comparison; 11 = Jack, 12 = Queen, 13 = King\n",
        "        random.shuffle(self.cards)\n",
        "        self.player_cards = self.cards[:2]\n",
        "\n",
        "    def is_terminal(self):\n",
        "        \"\"\"Checks if the game is at a terminal state.\"\"\"\n",
        "        return self.history in ['pp', 'bp', 'pbp', 'bb', 'pbb']\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns the list of available actions for the current game state.\"\"\"\n",
        "        if self.is_terminal():\n",
        "            return []  # No actions available if the game has ended\n",
        "        return ['b', 'p']\n",
        "\n",
        "    def perform_action(self, action):\n",
        "        \"\"\"Performs an action and updates the game state accordingly.\"\"\"\n",
        "        if self.is_terminal():  # Raise an error if an action is attempted when the game is terminal\n",
        "            raise ValueError(\"Cannot perform actions in a terminal game state.\")\n",
        "\n",
        "        self.history += action\n",
        "        if action == 'b':\n",
        "            self.bets[self.current_player] -= 1  # Deduct an additional unit for a bet\n",
        "        self.current_player = 1 - self.current_player  # Switch turns\n",
        "\n",
        "    def is_showdown(self):\n",
        "        \"\"\"Determines if the current state is a showdown.\"\"\"\n",
        "        if not self.is_terminal():\n",
        "            raise ValueError(\"Showdown status requested but the game is not in a terminal state.\")\n",
        "        return self.history[-1] == self.history[-2]\n",
        "\n",
        "    def get_payoff(self):\n",
        "        \"\"\"Calculates and returns the payoff based on the current game state without altering the game's state.\"\"\"\n",
        "        # Initialize a copy of the bets to calculate the payoff\n",
        "        payoff = self.bets[:]\n",
        "        if self.is_showdown():\n",
        "            winner = 0 if self.player_cards[0] > self.player_cards[1] else 1\n",
        "            loser = 1 - winner\n",
        "            payoff[winner] = -self.bets[winner]  # Invert the winner's negative bets to reflect the gain\n",
        "            payoff[loser] = self.bets[loser]     # Loser's bets stay negative\n",
        "        else:\n",
        "            winner = self.current_player\n",
        "            loser = 1 - winner\n",
        "            payoff[winner] = -self.bets[loser]   # Winner gets positive the amount lost by loser\n",
        "            payoff[loser] = self.bets[loser]     # Loser's bet remains the same (negative)\n",
        "        return payoff\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"Creates a deep copy of the current game state.\"\"\"\n",
        "        cloned_game = KuhnGame()\n",
        "        cloned_game.cards = self.cards[:]\n",
        "        cloned_game.player_cards = self.player_cards[:]\n",
        "        cloned_game.current_player = self.current_player\n",
        "        cloned_game.history = self.history[:]\n",
        "        cloned_game.bets = self.bets[:]\n",
        "        return cloned_game\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"Returns a string representation of the game state.\"\"\"\n",
        "        display_cards = [self.get_card_char(card) for card in self.player_cards]\n",
        "        return (f\"Game(cards={display_cards}, next_player_to_act={self.current_player + 1}, \"\n",
        "                f\"history='{self.history}', bets={self.bets})\")"
      ],
      "metadata": {
        "id": "A6lJUm-0hN9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract Base Class: Agent\n",
        "\n",
        "The `Agent` class serves as an abstract base class for game-playing agents. It defines the core functionalities that any agent must implement to interact with a game environment. The key responsibilities of this class include:\n",
        "\n",
        "- **Action Selection**: Defines how an agent chooses an action based on the current state of the game.\n",
        "\n",
        "This structure allows for the creation of various types of game-playing agents, providing flexibility in implementing different strategies or AI algorithms.\n"
      ],
      "metadata": {
        "id": "k8OjdxmUh-Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Agent(ABC):\n",
        "    @abstractmethod\n",
        "    def select_action(self, game):\n",
        "        \"\"\"\n",
        "        Select an action to take based on the current game state.\n",
        "\n",
        "        Parameters:\n",
        "            game (KuhnGame): The Kuhn Poker game in its current state.\n",
        "\n",
        "        Returns:\n",
        "            str: The action chosen by the agent. Possible actions are 'b' for bet and 'p' for pass.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "x2G7h-KOiGa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Agent\n",
        "\n",
        "Building upon the base `Agent` class, the `LearningAgent` class introduces additional functionalities pertinent to agents that can learn from their experiences. This class emphasizes methods that support learning and adaptation over time, essential for developing sophisticated AI players. Key features include:\n",
        "\n",
        "- **Experience Recording**: Captures the outcomes of actions taken in specific game states, facilitating learning mechanisms such as reinforcement learning or Monte Carlo methods.\n",
        "- **Training**: Iteratively adjusts the agent's strategy based on accumulated data, improving decision-making over time.\n",
        "- **Model Management**: Provides methods for saving and loading learned models, enabling persistence of learning across sessions.\n",
        "\n",
        "These enhancements make `LearningAgent` suitable for creating agents capable of evolving their strategies through gameplay.\n"
      ],
      "metadata": {
        "id": "FHOBnWljiZm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "\n",
        "class LearningAgent(Agent):\n",
        "    @abstractmethod\n",
        "    def record_experience(self, infoset, rewards):\n",
        "        \"\"\"\n",
        "        Record the experience associated with a specific game state to learn from it. Each possible action at this state\n",
        "        has an associated reward.\n",
        "\n",
        "        Parameters:\n",
        "            infoset (str): The representation of the game state, encapsulating the player's card and action history.\n",
        "            rewards (dict): A dictionary of rewards where keys are actions and values are the associated rewards for these actions.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, iterations):\n",
        "        \"\"\"\n",
        "        Train the agent based on accumulated data.\n",
        "\n",
        "        Parameters:\n",
        "            iterations (int): The number of training cycles to perform.\n",
        "\n",
        "        This method could be implemented to periodically adjust the agent's strategy, such as after a set number of games or rounds, or in response to performance metrics.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the model or strategy parameters to a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): The path to the file where the model should be saved.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Load the model or strategy parameters from a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): The path to the file from which the model should be loaded.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "W9QX2DsyiGQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chance Sampling CFR Agent\n",
        "\n",
        "The `CFRAgent` implements the Chance Sampling method of the Counterfactual Regret Minimization (CFR) algorithm. This method is designed to reduce computational complexity by sampling outcomes at chance nodes during the traversal of the game tree. Despite the sampling at chance nodes, the agent fully explores all possible actions available to the players at decision nodes, making it a comprehensive approach to learning optimal strategies in two-player zero-sum games.\n",
        "\n",
        "#### Key Features of the `CFRAgent`:\n",
        "- **Chance Sampling CFR**: Utilizes a chance-sampled game tree to minimize computation time per iteration while maintaining effective exploration of strategic options.\n",
        "- **Exhaustive Player Action Exploration**: At each decision node, all potential actions are considered, ensuring thorough evaluation of possible strategies.\n",
        "- **Adaptive Strategy Improvement**: Continuously refines strategies based on accumulated regret values for each action, progressively approaching an optimal Nash equilibrium strategy.\n",
        "- **Broad Applicability**: While specifically effective in two-player zero-sum games, the principles underpinning this agent are applicable across various game types that feature multiple agents and sequential decision-making.\n",
        "\n",
        "#### Operation Details:\n",
        "- **Strategy and Regret Management**: The agent keeps track of regret values for each action at every information set, which guide the adjustment of action probabilities in future iterations based on past performance.\n",
        "- **Iterative Strategy Optimization**: The agent adjusts its strategic approach by increasing the probabilities of actions with higher regret values, thus refining its strategy towards the Nash equilibrium over numerous iterations.\n",
        "- **Efficiency in Complex Environments**: By employing Chance Sampling, the `CFRAgent` efficiently manages computational resources while thoroughly exploring strategic options, making it suitable for complex, multi-agent environments with imperfect information.\n",
        "\n",
        "This implementation of Chance Sampling CFR offers a blend of computational efficiency and strategic depth, enabling the development of robust game strategies in environments characterized by uncertainty and multiple decision stages.\n"
      ],
      "metadata": {
        "id": "PYSwpRNXiY14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class CFRAgent(LearningAgent):\n",
        "    def __init__(self, game, delay=1000):\n",
        "        super().__init__()\n",
        "        self.game = game\n",
        "        self.regret_sum = {}\n",
        "        self.strategy_sum = {}\n",
        "        self.delay = delay\n",
        "        self.iteration_count = 0\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        # Ensure the 'models' directory exists\n",
        "        models_dir = 'models'\n",
        "        if not os.path.exists(models_dir):\n",
        "            os.makedirs(models_dir)\n",
        "        # Construct the full filepath\n",
        "        filepath = os.path.join(models_dir, filename)\n",
        "\n",
        "        data = {\n",
        "            'regret_sum': self.regret_sum,\n",
        "            'strategy_sum': self.strategy_sum,\n",
        "            'iteration_count': self.iteration_count\n",
        "        }\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        # Construct the full filepath\n",
        "        filepath = os.path.join('models', filename)\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"No model file found at {filepath}. Starting from scratch.\")\n",
        "            return False\n",
        "        try:\n",
        "            with open(filepath, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            self.regret_sum = data['regret_sum']\n",
        "            self.strategy_sum = data['strategy_sum']\n",
        "            self.iteration_count = data['iteration_count']\n",
        "            print(f\"Model loaded from {filepath}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load model from {filepath}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def select_action(self, game_state):\n",
        "        info_set = self.get_information_set(game_state)\n",
        "        available_actions = game_state.get_available_actions()\n",
        "        average_strategy = self.get_average_strategy(info_set, available_actions)\n",
        "        actions, weights = zip(*average_strategy.items())\n",
        "        action = random.choices(actions, weights=weights)[0]\n",
        "        return action\n",
        "\n",
        "    def get_average_strategy(self, information_set, available_actions):\n",
        "        strategy_sum = self.strategy_sum.get(information_set, {action: 1.0 for action in available_actions})\n",
        "        sum_of_sums = sum(strategy_sum.values())\n",
        "        return {action: strategy_sum[action] / sum_of_sums if sum_of_sums > 0 else 1.0 / len(available_actions) for action in available_actions}\n",
        "\n",
        "    def get_current_strategy(self, information_set, available_actions):\n",
        "        if information_set not in self.regret_sum:\n",
        "            self.regret_sum[information_set] = {action: 0 for action in available_actions}\n",
        "        regrets = self.regret_sum[information_set]\n",
        "        total_positive_regrets = sum(max(regret, 0) for regret in regrets.values())\n",
        "        return {action: (max(regrets[action], 0) / total_positive_regrets if total_positive_regrets > 0 else 1.0 / len(available_actions)) for action in available_actions}\n",
        "\n",
        "    def update_strategy_sums(self, information_set, strategy, reach_prob):\n",
        "        if self.iteration_count > self.delay:  # Only update strategy sums after the delay period\n",
        "            if information_set not in self.strategy_sum:\n",
        "                self.strategy_sum[information_set] = {action: 0 for action in strategy}\n",
        "            for action in strategy:\n",
        "                self.strategy_sum[information_set][action] += strategy[action] * reach_prob\n",
        "\n",
        "    def cfr(self, game_state, reach_prob):\n",
        "        if game_state.is_terminal():\n",
        "            return game_state.get_payoff()\n",
        "\n",
        "        hero = game_state.current_player\n",
        "        villain = 1 - hero\n",
        "        info_set = self.get_information_set(game_state)\n",
        "        available_actions = game_state.get_available_actions()\n",
        "        current_strategy = self.get_current_strategy(info_set, available_actions)\n",
        "        self.update_strategy_sums(info_set, current_strategy, reach_prob[villain])\n",
        "\n",
        "        node_utility = [0.0, 0.0]\n",
        "        action_utility = {}\n",
        "        for action in available_actions:\n",
        "            next_game_state = game_state.clone()\n",
        "            next_game_state.perform_action(action)\n",
        "            next_reach_prob = reach_prob[:]\n",
        "            next_reach_prob[hero] *= current_strategy[action]\n",
        "            action_utility[action] = self.cfr(next_game_state, next_reach_prob)[hero]\n",
        "            node_utility[hero] += current_strategy[action] * action_utility[action]\n",
        "\n",
        "        for action in available_actions:\n",
        "            regret = action_utility[action] - node_utility[hero]\n",
        "            self.regret_sum[info_set][action] += regret * reach_prob[villain]\n",
        "\n",
        "        node_utility[villain] = -node_utility[hero]\n",
        "\n",
        "        return node_utility\n",
        "\n",
        "    def train(self, iterations):\n",
        "        for i in range(iterations):\n",
        "            self.game.reset()\n",
        "            self.cfr(self.game, [1.0, 1.0])\n",
        "            self.iteration_count += 1  # Increment the iteration count after each training game\n",
        "\n",
        "    def get_information_set(self, game_state):\n",
        "        card_int = game_state.player_cards[game_state.current_player]\n",
        "        info_set = f\"{card_int}{game_state.history}\"\n",
        "        return info_set\n",
        "\n",
        "    def print_pretty_strategy(self):\n",
        "        print(\"Information Set | Action Probabilities\")\n",
        "        print(\"-\" * 41)\n",
        "        sorted_info_sets = sorted(self.strategy_sum.items(), key=lambda x: (int(x[0][:2]), x[0][2:]))\n",
        "        for info_set, strategies in sorted_info_sets:\n",
        "            card_int = int(info_set[:2])\n",
        "            card_char = PokerGame.get_card_char(card_int)\n",
        "            history = info_set[2:]\n",
        "            sum_of_sums = sum(strategies.values())\n",
        "            formatted_strategies = \" | \".join(f\"{action}: {strategies[action] / sum_of_sums:.5f}\" if sum_of_sums > 0 else f\"{action}: {1.0 / len(strategies):.5f}\" for action in strategies)\n",
        "            print(f\"{card_char}{history:<14} | {formatted_strategies}\")\n",
        "\n",
        "    def record_experience(self, infoset, rewards):\n",
        "        # Placeholder function, not used in CFR directly\n",
        "        pass"
      ],
      "metadata": {
        "id": "EHixwuD0iGEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Counterfactual Regret Minimization (CFR) Agent for Kuhn Poker\n",
        "\n",
        "In this section, we initiate and train a `CFRAgent` specifically tailored for Kuhn Poker. The CFR agent utilizes the Chance Sampling technique to optimize its strategy towards a Nash equilibrium over iterative self-play. Below are the steps followed in training and evaluating the agent:\n",
        "\n",
        "1. **Initialize the Agent**:\n",
        "   - An instance of `CFRAgent` is created with Kuhn Poker as the game environment.\n",
        "\n",
        "2. **Model Loading**:\n",
        "   - The system attempts to load a pre-existing trained model from `kuhn_cfr_model.pkl`. If no model is found, training starts from scratch. This approach allows for continuous improvement of the agent as more training iterations can be built upon previously learned strategies.\n",
        "\n",
        "3. **Training Process**:\n",
        "   - The agent undergoes a specified number of training iterations; in this case, 1,000,000. Each iteration represents a self-play game where the agent plays against itself, refining its strategy based on the outcomes and improving its understanding of the game dynamics.\n",
        "\n",
        "4. **Strategy Visualization**:\n",
        "   - After training, the agent's strategy is displayed. This showcases how the agent's decision-making has evolved over the training iterations, reflecting a move towards optimal play.\n",
        "\n",
        "5. **Model Saving**:\n",
        "   - Post training, the current state of the agent (including its strategy sums and regret values) is saved back to `kuhn_cfr_model.pkl`. This model saving enables the preservation of strategic developments and provides a checkpoint that can be reloaded in future sessions.\n",
        "\n",
        "6. **Performance Reflection**:\n",
        "   - The total number of iterations completed during training is printed, providing a metric of the agent's experience and the computational effort involved.\n",
        "\n",
        "This methodology ensures a systematic approach to developing a robust CFR agent capable of approaching theoretical optimal strategies in Kuhn Poker through computational learning and iterative self-improvement.\n"
      ],
      "metadata": {
        "id": "_kQceS-6usEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kuhn_cfr_agent = CFRAgent(KuhnGame())\n",
        "\n",
        "# Try to load a pre-existing model\n",
        "if not kuhn_cfr_agent.load_model('kuhn_cfr_model.pkl'):\n",
        "    print(\"Starting training from scratch.\")\n",
        "\n",
        "# Train with a suitable number of iterations\n",
        "kuhn_cfr_agent.train(1000000)\n",
        "\n",
        "# Print strategies to see how they have evolved\n",
        "kuhn_cfr_agent.print_pretty_strategy()\n",
        "print(\"Iterations:\", kuhn_cfr_agent.iteration_count)\n",
        "\n",
        "# Save the model after training\n",
        "kuhn_cfr_agent.save_model('kuhn_cfr_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg_po7OLutl_",
        "outputId": "0e73d9dc-dfee-4d3d-c82e-983e8c52ed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from models/kuhn_cfr_model.pkl\n",
            "Information Set | Action Probabilities\n",
            "-----------------------------------------\n",
            "J               | b: 0.30828 | p: 0.69172\n",
            "Jb              | b: 0.00000 | p: 1.00000\n",
            "Jp              | b: 0.33348 | p: 0.66652\n",
            "Jpb             | b: 0.00000 | p: 1.00000\n",
            "Q               | b: 0.00000 | p: 1.00000\n",
            "Qb              | b: 0.33292 | p: 0.66708\n",
            "Qp              | b: 0.00000 | p: 1.00000\n",
            "Qpb             | b: 0.64086 | p: 0.35914\n",
            "K               | b: 0.92315 | p: 0.07685\n",
            "Kb              | b: 1.00000 | p: 0.00000\n",
            "Kp              | b: 1.00000 | p: 0.00000\n",
            "Kpb             | b: 1.00000 | p: 0.00000\n",
            "Iterations: 51000000\n",
            "Model saved to models/kuhn_cfr_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Response Agent for Testing CFR Strategies\n",
        "\n",
        "The `BestResponseAgent` is designed to identify and exploit the best responses to strategies developed by other agents, such as those trained via Counterfactual Regret Minimization (CFR). This agent plays a crucial role in validating the robustness and optimality of learned strategies by attempting to exploit any possible weaknesses systematically.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "- **Strategy Profile Maintenance**: Maintains a record of average rewards associated with each action at different game states (information sets). This allows the agent to choose actions that historically yielded the highest rewards.\n",
        "\n",
        "- **Action Selection**:\n",
        "  - For each decision point, the agent selects the action with the highest average reward based on past encounters within the same information set.\n",
        "  - If no historical data is available for the current state, it defaults to a random action selection, ensuring variability in play and exploration of new strategies.\n",
        "\n",
        "- **Experience Recording**:\n",
        "  - After each game, the agent records the outcomes associated with each action taken at various states to update its strategy profile.\n",
        "  - This continual learning process refines its understanding of the game dynamics and optimal responses.\n",
        "\n",
        "- **Model Persistence**:\n",
        "  - **Saving Models**: The agent's current strategy profile can be saved to a file, allowing long-term retention of learned strategies and easy reload for further refinement or evaluation.\n",
        "  - **Loading Models**: At initialization, the agent attempts to load a pre-existing strategy profile from a specified file. This feature facilitates cumulative learning where the agent resumes learning from where it left off.\n",
        "\n",
        "- **Pretty Print Strategy**:\n",
        "  - Provides a formatted output of the learned strategy profile, detailing the average rewards for each action at different information sets. This output is invaluable for analysis and debugging of the agent's decision-making process.\n",
        "\n",
        "#### Usage Scenario:\n",
        "\n",
        "The `BestResponseAgent` is typically used in a testing or simulation environment where it is tasked with playing against a pre-trained agent (like a CFR-based agent). By rigorously challenging the strategies of these trained agents, it helps in verifying their strength and identifying potential areas for improvement.\n",
        "\n",
        "### Implementation Note:\n",
        "\n",
        "This agent is part of a broader framework where it interacts with various game types and other agents. It is crucial for the testing phase of strategy development, providing insights into the effectiveness of different strategies under adversarial conditions.\n"
      ],
      "metadata": {
        "id": "vhrktEbKzSR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "class BestResponseAgent(LearningAgent):\n",
        "    def __init__(self, game):\n",
        "        self.game = game\n",
        "        self.strategy_profile = {}  # Store the strategy profiles with counters for averaging\n",
        "\n",
        "    def select_action(self, game):\n",
        "        hero_card = game.player_cards[game.current_player]\n",
        "        infoset = f\"{hero_card}{''.join(game.history)}\"\n",
        "\n",
        "        # Check if we have history and the strategy profile has this history recorded\n",
        "        if infoset in self.strategy_profile:\n",
        "            # Select the action with the highest average reward\n",
        "            best_action = max(self.strategy_profile[infoset], key=lambda a: self.strategy_profile[infoset][a]['total_reward'] / self.strategy_profile[infoset][a]['count'])\n",
        "            return best_action\n",
        "\n",
        "        # If no history or no data for this history, choose randomly\n",
        "        return random.choice(game.get_available_actions())\n",
        "\n",
        "    def record_experience(self, infoset, rewards):\n",
        "        if infoset not in self.strategy_profile:\n",
        "            self.strategy_profile[infoset] = {}\n",
        "\n",
        "        for action, reward in rewards.items():\n",
        "            if action not in self.strategy_profile[infoset]:\n",
        "                self.strategy_profile[infoset][action] = {'total_reward': 0, 'count': 0}\n",
        "            self.strategy_profile[infoset][action]['total_reward'] += reward\n",
        "            self.strategy_profile[infoset][action]['count'] += 1\n",
        "\n",
        "    def train(self, iterations=1):\n",
        "        # Training method to potentially adjust internal strategies based on observed gameplay\n",
        "        pass\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        # Ensure the 'models' directory exists\n",
        "        models_dir = 'models'\n",
        "        if not os.path.exists(models_dir):\n",
        "            os.makedirs(models_dir)\n",
        "        # Construct the full filepath\n",
        "        filepath = os.path.join(models_dir, filename)\n",
        "\n",
        "        try:\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(self.strategy_profile, f, ensure_ascii=False, indent=4)\n",
        "            print(\"Model saved successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save model: {e}\")\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        # Construct the full filepath\n",
        "        filepath = os.path.join('models', filename)\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(filepath):\n",
        "                with open(filepath, 'r') as f:\n",
        "                    self.strategy_profile = json.load(f)\n",
        "                print(\"Model loaded successfully.\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"No model found at {filepath}. Starting fresh.\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def print_pretty_strategy(self):\n",
        "        print(\"Information Set | Action Values\")\n",
        "        print(\"-\" * 38)\n",
        "        # Sorting the strategy profile by card integer value and then by the history\n",
        "        sorted_info_sets = sorted(self.strategy_profile.items(), key=lambda x: (int(x[0][:1]), x[0][1:]))\n",
        "        for info_set, actions in sorted_info_sets:\n",
        "            card_int = int(info_set[:2])  # Extract the card integer directly\n",
        "            card_char = PokerGame.get_card_char(card_int)  # Convert card integer to character\n",
        "            history = info_set[2:]  # Extract history part of the information set\n",
        "            actions_str = ' | '.join([f\"{a}: {data['total_reward'] / data['count']:.2f}\" for a, data in actions.items() if data['count'] > 0])\n",
        "            print(f\"{card_char}{history:<14} | {actions_str}\")\n"
      ],
      "metadata": {
        "id": "gEWEzhkauyiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gameplay Simulator\n",
        "\n",
        "The `GameplaySimulator` is designed to simulate gameplay between two agents, referred to as `hero` and `villain`, within a defined game setting like Kuhn or Leduc Poker. This class facilitates the interaction of strategies and tactics between competing agents, allowing for the dynamic training and assessment of agent behaviors.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "- **Agent Interaction**:\n",
        "  - The simulator controls the sequence of gameplay, determining which agent takes action based on the game state and alternating control between the `hero` and `villain`.\n",
        "  \n",
        "- **Exploratory Game Play**:\n",
        "  - Each game is played to completion by recursively deciding actions for each agent based on the current game state until a terminal condition is met (e.g., all rounds are complete or a player folds).\n",
        "  \n",
        "- **Information Set Construction**:\n",
        "  - During gameplay, an information set for the `hero` is constructed that encapsulates the current state from the `hero`'s perspective. This typically includes the hero's own cards and the action history, forming the basis for decision-making.\n",
        "\n",
        "- **Adaptability Note**:\n",
        "  - The current implementation of the `GameplaySimulator` uses only one hero card for constructing the information set, which is adequate for games like Kuhn Poker. For games with more complex or multiple hole cards, this approach may need to be adapted to ensure a more comprehensive representation of the game state is considered.\n",
        "\n",
        "- **Action Reward Mapping**:\n",
        "  - For every action available at a particular state, the simulator clones the game state, performs the action, and recursively evaluates the outcome. This approach captures the direct consequences of actions, informing the hero's strategy adjustments.\n",
        "  \n",
        "- **Strategy Updates**:\n",
        "  - After exploring all possible actions and their outcomes, the simulator updates the `hero`'s strategy based on the observed rewards. This continuous feedback loop enhances the `hero`'s decision-making process over multiple simulations.\n",
        "\n",
        "- **Reward Return**:\n",
        "  - The simulator returns the reward of the chosen action, aiding in the assessment of strategy efficacy and decision-making processes over successive games.\n",
        "\n",
        "#### Usage Scenario:\n",
        "\n",
        "The `GameplaySimulator` is integral to training scenarios where a `hero` agent is tested against a `villain` to refine strategies or to evaluate the robustness of a learned strategy against a predefined or adaptive adversary. It is particularly useful in environments where exhaustive exploration of game states is feasible and informative for agent learning.\n",
        "\n",
        "### Implementation Note:\n",
        "\n",
        "While designed to be generic enough to handle any two-player card game, the `GameplaySimulator` is optimized for games like Kuhn and Leduc Poker, where the complexity and depth of the game tree allow for comprehensive strategy development and testing. This class is a cornerstone of the training framework, ensuring agents can adapt and optimize their strategies in a controlled yet competitive setting.\n"
      ],
      "metadata": {
        "id": "XgGrhiH10FYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GameplaySimulator:\n",
        "    def __init__(self, hero, villain):\n",
        "        self.hero = hero\n",
        "        self.villain = villain\n",
        "\n",
        "    def explore_game(self, game, hero_player):\n",
        "        if game.is_terminal():\n",
        "            return game.get_payoff()[hero_player]\n",
        "\n",
        "        current_player = game.current_player\n",
        "\n",
        "        if current_player == hero_player:\n",
        "            hero_card = game.player_cards[hero_player]  # Include hero's card in history\n",
        "            infoset = f\"{hero_card}{''.join(game.history)}\"  # Create a infoset string with integer card value\n",
        "\n",
        "            available_actions = game.get_available_actions()\n",
        "            action_rewards = {}\n",
        "\n",
        "            for action in available_actions:\n",
        "                game_clone = game.clone()\n",
        "                game_clone.perform_action(action)\n",
        "                reward = self.explore_game(game_clone, hero_player)\n",
        "                action_rewards[action] = reward\n",
        "\n",
        "            self.hero.record_experience(infoset, action_rewards)\n",
        "\n",
        "            # Return the reward for the action chosen by the hero's strategy\n",
        "            return action_rewards[self.hero.select_action(game)]\n",
        "        else:\n",
        "            action = self.villain.select_action(game)\n",
        "            game.perform_action(action)\n",
        "            return self.explore_game(game, hero_player)"
      ],
      "metadata": {
        "id": "QP1_34St0F8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Best Response Agent\n",
        "\n",
        "The Best Response Agent is designed to optimize its strategy against a pre-trained opponent, in this case, a Counterfactual Regret Minimization (CFR) agent. This section outlines the steps involved in training, loading, and saving the Best Response Agent, ensuring it adapts effectively to the strategies employed by the CFR agent.\n",
        "\n",
        "#### Initial Setup\n",
        "\n",
        "1. **Agent Initialization**:\n",
        "   - A new instance of the Best Response Agent is created, tailored for the game of Kuhn Poker.\n",
        "\n",
        "2. **Model Loading**:\n",
        "   - The agent attempts to load a pre-existing model from a file (`best_response_model.pkl`). If the model exists, it is loaded into the agent, allowing it to continue refining its strategy from previous training sessions. If no model is found, training starts from scratch.\n",
        "\n",
        "#### Training Process\n",
        "\n",
        "3. **Gameplay Simulation**:\n",
        "   - A `GameplaySimulator` is set up with the Best Response Agent as the hero and the pre-trained CFR agent as the villain. This setup is crucial for ensuring that the Best Response Agent learns to counter strategies that the CFR agent might employ.\n",
        "   - The agent undergoes training through repeated game sessions, where each session involves:\n",
        "     - Resetting the game to a clean state.\n",
        "     - Simulating a complete game from the perspective of the Best Response Agent, allowing it to explore different actions and learn from the outcomes.\n",
        "\n",
        "4. **Exploring Game States**:\n",
        "   - For each training iteration, the game is played twiceâ€”once with the Best Response Agent starting the game and once with the CFR agent starting. This alternating start helps in providing balanced exposure to different game states, enhancing the learning and adaptability of the Best Response Agent.\n",
        "\n",
        "#### Strategy Evaluation and Saving\n",
        "\n",
        "5. **Strategy Output**:\n",
        "   - Post-training, the strategy of the Best Response Agent is printed, showcasing how it decides to act in various situations based on the learned experiences.\n",
        "\n",
        "6. **Model Saving**:\n",
        "   - The newly trained model is saved back to `best_response_model.pkl`, preserving the learning and improvements made during the training sessions for future use.\n",
        "\n",
        "This training approach ensures that the Best Response Agent develops a robust strategy to effectively counter the moves of a CFR-trained opponent in Kuhn Poker. By continually refining its responses and strategies through simulated gameplay against a sophisticated adversary, the agent enhances its competitiveness and readiness for real-world application.\n"
      ],
      "metadata": {
        "id": "tDWRm7ck1q1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming CFR agent is already trained and ready\n",
        "best_response_agent = BestResponseAgent(KuhnGame())  # Initialize Best Response agent for training\n",
        "\n",
        "# Load the best response model if available\n",
        "model_filename = 'best_response_model.pkl'\n",
        "if best_response_agent.load_model(model_filename):\n",
        "    print(\"Loaded existing best response model.\")\n",
        "else:\n",
        "    print(\"No existing model found, starting fresh training.\")\n",
        "\n",
        "gameplay_simulator = GameplaySimulator(best_response_agent, kuhn_cfr_agent)\n",
        "\n",
        "# Train the Best Response Agent by playing it against the CFR Agent\n",
        "game = KuhnGame()  # Create a new game instance for the training session\n",
        "for _ in range(1000000):\n",
        "    game.reset()\n",
        "    gameplay_simulator.explore_game(game, 0)\n",
        "    game.reset()\n",
        "    gameplay_simulator.explore_game(game, 1)\n",
        "\n",
        "# Save the newly trained best response model\n",
        "best_response_agent.save_model(model_filename)\n",
        "\n",
        "# Print strategy of the Best Response Agent\n",
        "best_response_agent.print_pretty_strategy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fz36R_K1rf4",
        "outputId": "c341f6b3-7429-4bb3-e9bd-8c814bbcd707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Loaded existing best response model.\n",
            "Model saved successfully.\n",
            "Information Set | Action Values\n",
            "--------------------------------------\n",
            "J               | b: -1.00 | p: -1.00\n",
            "Jb              | b: -2.00 | p: -1.00\n",
            "Jp              | b: -1.00 | p: -1.00\n",
            "Jpb             | b: -2.00 | p: -1.00\n",
            "Q               | b: -0.50 | p: -0.33\n",
            "Qb              | b: -1.00 | p: -1.00\n",
            "Qp              | b: 0.70 | p: 0.80\n",
            "Qpb             | b: -1.00 | p: -1.00\n",
            "K               | b: 1.17 | p: 1.17\n",
            "Kb              | b: 2.00 | p: -1.00\n",
            "Kp              | b: 1.38 | p: 1.00\n",
            "Kpb             | b: 2.00 | p: -1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation of CFR and Best Response Agents in Kuhn Poker\n",
        "\n",
        "In this section, we simulate a large number of Kuhn Poker games between a pre-trained Counterfactual Regret Minimization (CFR) agent and a Best Response agent. The simulation helps verify the effectiveness of the CFR strategy by alternately positioning the CFR agent as Player 0 and Player 1 against an adaptive opponent. This approach provides insights into the CFR agent's performance and adaptability under varied game conditions.\n",
        "\n",
        "### Setup\n",
        "- **Game Initialization**: We use the `KuhnGame` class to create instances of the game.\n",
        "- **Agent Initialization**: The CFR agent is assumed to be pre-trained and is ready for testing. The Best Response agent is also initialized.\n",
        "- **Simulation Parameters**:\n",
        "  - `num_games`: The total number of games played, set to 1,000,000 to ensure robust statistical results.\n",
        "  - `payoff_records`: An array to track the cumulative payoffs for the CFR agent when it plays as Player 0 and Player 1.\n",
        "\n",
        "### Simulation Process\n",
        "1. **CFR as Player 0**:\n",
        "   - For each game instance, the CFR agent acts as Player 0. The game progresses turn by turn between the CFR and Best Response agents until a terminal state (end of the game) is reached.\n",
        "   - Payoffs are recorded for the CFR agent at the end of each game.\n",
        "   \n",
        "2. **CFR as Player 1**:\n",
        "   - The roles are reversed, and the process is repeated with the CFR agent acting as Player 1. This setup tests the agent's strategies from both player perspectives.\n",
        "\n",
        "### Calculation of Results\n",
        "- **Average Payoff**:\n",
        "  - The average payoff for the CFR agent when it plays as Player 0 and Player 1 is calculated by dividing the total payoffs by the number of games. This measure evaluates the agent's overall performance against the Best Response agent.\n",
        "- **Theoretical Comparison**:\n",
        "  - According to game theory and prior analysis of Kuhn Poker, the expected average utility for Player 0 is -1/18 and for Player 1 is 1/18 under optimal play. Our results are compared against these benchmarks to assess the CFR agent's adherence to theoretically optimal strategies. For further reading on Kuhn Poker strategy and Nash equilibrium, visit [Wikipedia](https://en.wikipedia.org/wiki/Kuhn_poker).\n",
        "\n",
        "The results from this simulation provide valuable insights into the strategic consistency and effectiveness of the CFR agent when confronted with a strategically optimized opponent. It also serves as a practical validation of the CFR algorithm's capability to approximate Nash equilibrium strategies in two-player zero-sum games like Kuhn Poker.\n",
        "\n",
        "### Implementation Code\n",
        "Here is the code snippet that sets up and runs the simulation, evaluates the outcomes, and prints the average payoffs:\n"
      ],
      "metadata": {
        "id": "gDIFt-pQ3A7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Initialize the game and agents\n",
        "game = KuhnGame()\n",
        "num_games = 1000000\n",
        "payoff_records = [0, 0]\n",
        "\n",
        "# Simulation loop for a large number of games\n",
        "for game_index in range(num_games):\n",
        "    # Simulate game where CFR is Player 0\n",
        "    game.reset()\n",
        "    while not game.is_terminal():\n",
        "        current_player = kuhn_cfr_agent if game.current_player == 0 else best_response_agent\n",
        "        action = current_player.select_action(game)\n",
        "        game.perform_action(action)\n",
        "    payoffs = game.get_payoff()\n",
        "    payoff_records[0] += payoffs[0]\n",
        "\n",
        "    # Simulate game where CFR is Player 1\n",
        "    game.reset()\n",
        "    while not game.is_terminal():\n",
        "        current_player = best_response_agent if game.current_player == 0 else kuhn_cfr_agent\n",
        "        action = current_player.select_action(game)\n",
        "        game.perform_action(action)\n",
        "    payoffs = game.get_payoff()\n",
        "    payoff_records[1] += payoffs[1]\n",
        "\n",
        "# Calculate average payoffs\n",
        "average_payoff_0 = payoff_records[0] / num_games\n",
        "average_payoff_1 = payoff_records[1] / num_games\n",
        "print(f\"Average Payoff over {num_games} games: CFR as Player 0 -> {average_payoff_0:.5f}, CFR as Player 1 -> {average_payoff_1:.5f}\")\n",
        "print(f\"Expected theoretical payoff for Player 0: {-1/18:.5f}, for Player 1: +{1/18:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNX65zOY3BX7",
        "outputId": "5e0c2845-67c6-44ee-d2d1-eff03b96a4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Payoff over 1000000 games: CFR as Player 0 -> -0.05415, CFR as Player 1 -> 0.05602\n",
            "Expected theoretical payoff for Player 0: -0.05556, for Player 1: +0.05556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "The simulation of 1,000,000 games of Kuhn Poker provided a comprehensive evaluation of the Counterfactual Regret Minimization (CFR) agent against a Best Response agent. Here's what we've learned and what's next:\n",
        "\n",
        "### Observations:\n",
        "- **Performance Consistency**: The CFR agent demonstrated its ability to perform close to the theoretical expectations derived from game theory, effectively approximating Nash equilibrium strategies in Kuhn Poker.\n",
        "- **Effectiveness of CFR**: The results confirm the effectiveness of the CFR algorithm in achieving outcomes that align closely with theoretical predictions, showcasing its reliability in strategic game settings.\n",
        "\n",
        "### Future Work:\n",
        "- **Exploring More Complex Games**: To further validate the algorithm's effectiveness, the next step involves applying CFR to Leduc Holdem, a more complex variant of poker that introduces greater decision-making complexity. [Placeholder for future Leduc Holdem Notebook](#).\n",
        "- **Integration with Neural Networks**: There is also an interest in integrating neural networks with CFR strategies. This initiative will explore whether combining these technologies can enhance or even surpass the strategic decision-making capabilities currently achieved by traditional CFR methods.\n",
        "\n",
        "These initiatives aim to deepen our understanding of game theory applications in AI, expanding their practical utility across a wider range of strategic games and decision-making environments.\n"
      ],
      "metadata": {
        "id": "OO2Ufge-BPZU"
      }
    }
  ]
}